{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bm}{\\boldsymbol}\n",
    "\\newcommand{\\Pr}{\\mathbb{P}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\T}{\\top}\n",
    "\\DeclareMathOperator{\\Var}{Var}\n",
    "\\DeclareMathOperator{\\Cov}{Cov}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number the equations (TeX/MathJax)\n",
    "# display(\"text/html\", \"<script type=\\\"text/javascript\\\">MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber : \\\"AMS\\\" }} })</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install some packages\n",
    "# plotting (Gadfly and Compose)\n",
    "Pkg.add(\"Gadfly\")\n",
    "Pkg.add(\"Compose\")\n",
    "# optimization\n",
    "Pkg.add(\"NLopt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gadfly\n",
    "using Compose\n",
    "set_default_plot_size(12cm, 12cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed later, can ignore for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an update in place operator\n",
    "function (←){T}(a::T, b::T)\n",
    "    for f in fieldnames(a)\n",
    "        setfield!(a, f, getfield(b, f))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian processes with Julia\n",
    "\n",
    "The aim of this session is to introduce Gaussian process regression, and show how it might be implemented in Julia.  I haven't considered Gaussian process classification (but see the Resources section).\n",
    "\n",
    "Currently, Julia is gradually settling on its core language feature set and syntax, so expect some breaking changes until Julia v1.0.  This notebook was written for Julia v0.6.\n",
    "\n",
    "Although this isn't a language tutorial, I have tried to explain some of the less-obvious semantics and features.\n",
    "\n",
    "- https://julialang.org/\n",
    "- https://docs.julialang.org/en/stable/\n",
    "- https://github.com/JuliaLang/julia\n",
    "\n",
    "\n",
    "## Introduction and Notation\n",
    "\n",
    "Suppose we have some data\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\{(\\bm{x}^{(i)}, t^{(i)})\\}_{i=1}^N\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\bm{x}^{(i)}$ is the _i_-th (vector) _input_ and $t^{(i)}$ is the corresponding (scalar) _observation_.  It is often convenient to write $\\bm{X} = (\\bm{x}^{(i)})_{i=1}^N$ and $\\bm{t} = (t^{(i)})_{i=1}^N$.\n",
    "\n",
    "We wish to construct a model so that given a new input $\\bm{x}'$ we can make a prediction of the corresponding $t'$.  That is to say, we wish to find the posterior probability distribution of $t'$, given $\\bm{x}'$ and the other observations:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\Pr(t' \\mid \\bm{x}', \\{(\\bm{x}^{(i)}, t^{(i)})\\}_{i=1}^N) \\equiv \\Pr(t' \\mid \\bm{x}', \\bm{X}, \\bm{t}). \\qquad (\\star)\n",
    "\\label{eq:cond-distr}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This is just the familiar problem of function fitting or regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional distribution, $(\\star)$, can be expressed straightforwardly in terms of the joint distribution,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\Pr(t' \\mid \\bm{x}', \\bm{X}, \\bm{t}) = \\frac{\\Pr(\\bm{t},t'\\mid \\bm{X},\\bm{x}')}{\\Pr(\\bm{t} \\mid \\bm{X})}. \\qquad (\\clubsuit)\n",
    "\\label{eq:cond-distr2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Supposing that the $t^{(i)}$ are drawn from a _Gaussian process_ (defined in the next section) allows us to evaluate the right-hand side of this expression exactly.  We will see that this is equivalent to inferring a functional model for our data (perhaps with noise), with a particular prior on the space of functions.\n",
    "\n",
    "## Gaussian processes\n",
    "\n",
    "A **Gaussian process** is a stochastic process $(t^{(n)})_{n \\in I}$, with $n$ ranging over an index set $I$, where any finite collection of variables $\\bm{t} = (t^{(n_1)}, t^{(n_2)}, \\ldots, t^{(n_N)})$ has a joint Gaussian distribution:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\Pr(\\bm{t}\\mid \\bm\\mu, \\bm{C}) = \\frac{1}{Z} \\exp \\left( -\\half (\\bm{t} - \\bm\\mu )^\\T \\bm{C}^{-1} (\\bm{t} - \\bm\\mu ) \\right)\\\\\n",
    "Z = \\left. 1 \\middle/ \\sqrt{(2\\pi)^N\\det{C}}. \\right.\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "The $\\bm{C}$ in this equation is unspecified, and we will see that a central problem for a modelling task is to choose it well.  We will most often have $\\bm\\mu = \\bm{0}$.  The components of $\\bm{C}$ are usually functions of the input coordinates $\\bm{X}$.  Once we know how to calculate $\\bm{C}$, we can compute the quantity appearing on the RHS of equation $(\\clubsuit)$.  We show how this is done in **Making predictions**.\n",
    "\n",
    "### Simple example\n",
    "\n",
    "To illustrate the idea, we consider a simple example with two variables, one a known input and one to predict.\n",
    "\n",
    "Suppose these are $t_1$ and $t_2$, and as above, we know that they have a joint Gaussian distribution.  Suppose also that we know that these have $\\bm\\mu = \\bm{0}$ and a given covariance matrix $\\bm C$ shown below.\n",
    "\n",
    "In this scenario, if we know the value of $t_1$, we can compute the conditional distribution of the unknown $t_2$.  Relating this to the previous section, $t_1$ plays the role of the known observations ($\\bm{t}$), and $t_2$ is the output we wish to infer ($t'$).\n",
    "\n",
    "Since we specify $\\bm{C}$ directly, we do not consider the $\\bm{x}$s in this example, although $\\bm{C}$ would normally be computed in terms of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [1.00 0.95;\n",
    "     0.95 1.00]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines the pdf of a joint Gaussian in two variables, with a given covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Julia features: import, named parameters, matrix division, unpacking\n",
    "\n",
    "\"Pdf of a joint Gaussian distribution in two variables.\"\n",
    "function gaussian_pdf(x,y; cov_matrix=C)\n",
    "    # the \\ operator is linear system solve (similar to Matlab).\n",
    "    return exp(-0.5*dot([x,y],cov_matrix\\[x,y])) / (2*pi) * sqrt(det(cov_matrix))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a conditional distribution as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quadgk: numerical integration in 1-d\n",
    "import QuadGK: quadgk\n",
    "\n",
    "\"\"\"\n",
    "    conditional_distr(f :: Function, condition :: Function)\n",
    "\n",
    "Given a distribution `f`, representing a pdf ``f_{X,Y}(x,y)``, and a \n",
    "condition `condition` of the form `y -> (a,y)`, returns a function of\n",
    "one argument representing ``f_{Y|X=a}(y)``.  Limited to two variables \n",
    "due to quadgk.\n",
    "\"\"\"\n",
    "function conditional_distr(f, condition)\n",
    "    marginal, error = quadgk(x -> f(condition(x)...), -Inf, Inf)\n",
    "    return (x -> f(condition(x)...)/marginal)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string just before the function declaration is an optional docstring.  Documentation can be accessed in the REPL and in a Jupyter notebook with `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?conditional_distr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot the joint density, and the conditional distribution with our known $t_0 = 1$.  This is done using [Gadfly.jl](http://gadflyjl.org/stable/index.html), a plotting library inspired by \"The Grammar of Graphics\" and ggplot2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Julia features: let, plotting with Gadfly\n",
    "\n",
    "let t1s = linspace(-2.0, 2.0, 200),\n",
    "    t2s = linspace(-2.0, 2.0, 200),\n",
    "    plot_range = Coord.cartesian(xmin=minimum(t1s),\n",
    "                                 xmax=maximum(t1s),\n",
    "                                 ymin=minimum(t2s),\n",
    "                                 ymax=maximum(t2s)),\n",
    "    condition_annotation = Guide.annotation(compose(context(), # new empty context for the composed annotation\n",
    "                                                    line([(1.0,10.0), (1.0,-10.0)]),\n",
    "                                                    stroke(\"violet\"))),\n",
    "    contour_levels = Geom.contour(levels=20);\n",
    "    \n",
    "    draw(SVGJS(24cm, 12cm),\n",
    "         hstack(plot(x = t1s,\n",
    "                     y = t2s,\n",
    "                     z = [gaussian_pdf(t1,t2) for t1 in t1s, t2 in t2s],\n",
    "                     plot_range,\n",
    "                     contour_levels,\n",
    "                     condition_annotation,\n",
    "                     Guide.xlabel(\"t₁\"),\n",
    "                     Guide.ylabel(\"t₂\")),\n",
    "                plot(x = [conditional_distr(gaussian_pdf, y->(1,y))(t1) for t1 in t1s],\n",
    "                     y = t1s,\n",
    "                     Guide.xlabel(\"p(t₂)\"),\n",
    "                     Guide.ylabel(\"t₂\"))))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right-hand plot gives the predictive distribution of $t_2$, given $t_1$, based on the covariance matrix above.  This is just another Gaussian of course, with\n",
    "\n",
    "$$\n",
    "t_2 \\sim N\\left(\\frac{C_{12}}{C_{11}}t_1,\\; C_{22} - \\frac{C_{12}^2}{C_{11}}\\right).\n",
    "$$\n",
    "\n",
    "This is preciesly $(\\clubsuit)$ in our two variable case.  Making predictions of more than one variable, from more than one input point is just the obvious extension of this idea to more than two variables, as we shall see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another perspective\n",
    "\n",
    "Suppose we believe that our observations $t$ arise from sampling from a function $y(\\bm{x})$, perhaps with some noise.  We then express our inference problem as that of determining the posterior distribution of the function $y(\\bm{x})$ given the observations.  Using Bayes' rule, we get\n",
    "\n",
    "$$ \n",
    "\\Pr(y(\\bm{x}) \\mid \\bm{t}, \\bm{X}) = \n",
    "\\frac{\\Pr(\\bm{t} \\mid y(\\bm{x}), \\bm{X})\\Pr(y(\\bm{x}))}{\\Pr(\\bm{t}\\mid\\bm{X})}.\n",
    "$$\n",
    "\n",
    "The first term in the numerator of the RHS represents the noise, which is often taken to be a separable multivariate Gaussian distribution.\n",
    "\n",
    "The second term in the numerator is the prior distribution of functions in our model.  We could specify a prior by first parameterizing $y$, but Gaussian processes allow us to put a prior directly on the space of functions.  This is done by stating that function evaluations form a Gaussian process, and by specifying the covariance for the underlying function for pairs of inputs, that is, we specify a $k$ such that\n",
    "\n",
    "$$\n",
    "\\Cov(y(\\bm{x}^{(1)}),y(\\bm{x}^{(2)})) = k(\\bm{x}^{(1)},\\bm{x}^{(2)}).\n",
    "$$\n",
    "\n",
    "We must clearly be careful what we mean by statements like \"putting a prior on the space of functions\".  This perspective can be formulated rigorously (which I haven't included here), and the two points of view—distributions of finite samples from a stochastic process and distributions on the function space—connected.  The relevant result is Komolgorov's existance theorem, aka the Daniell-Komolgorov theorem.\n",
    "\n",
    "## Covariance kernels\n",
    "\n",
    "As suggested above, the covariance kernel specifies how entries of the covariance matrix $\\bm{C}$ are calculated from the input coordinates.  In addition, the covariance matrix must incorporate the noise model.  We will exclusively use parameter-independent Gaussian noise, giving a covariance matrix with entries\n",
    "\n",
    "$$\n",
    "C_{ij} = k(\\bm{x}^{(i)}, \\bm{x}^{(j)}) + \\nu\\delta_{ij}.\n",
    "$$\n",
    "\n",
    "A very common choice of covariance kernel is the squared exponential, which is simply\n",
    "\n",
    "$$\n",
    "k_{\\text{sqexp}}(\\bm{x}^{(1)},\\bm{x}^{(2)}) = \\sigma^2\\exp\\left(-\\half\\sum_i \\frac{(x^{(1)}_i - x^{(2)}_i)^2}{r_i^2} \\right),\n",
    "$$\n",
    "or in Julia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_exp(x,y,σ,r) = σ^2 * exp(-sum(0.5*(x-y).^2/r.^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This expresses the idea that function values are more correlated when their arguments are close, and become rapidly decorrelated past a certain characteristic distance for each component, $r_i$.\n",
    "\n",
    "A careful choice of kernel can capture other known properties of the function, such as periodicity.\n",
    "\n",
    "The covariance matrix is required to be symmetric and non-negative definite.\n",
    "\n",
    "If the covariance function depends only on the separation ($\\bm{x} - \\bm{y}$), then it is said to be *stationary*, although this is not a requirement.\n",
    "\n",
    "\n",
    "## Making predictions\n",
    "\n",
    "Given our $N$ observations $\\{(\\bm{x}^{(i)}, t^{(i)})\\}_{i=1}^N$, a covariance kernel $k$ and a noise hyperparameter $\\nu$, and suppose that we want to make a prediction at a new point $\\bm{x}'$, which we call $t'$.\n",
    "\n",
    "To make this prediction, first compute the covariance matrix for all $N+1$ input coordinates, $\\bm{C}_{N+1}$, and then the conditional distribution is\n",
    "\n",
    "$$\n",
    "\\Pr(t'\\mid \\bm{t}, \\bm{X}, \\bm{x}') = \\frac{1}{Z} \\exp \\left( -\\half \\left[\\bm{t}\\; t'\\right]\\bm{C}_{N+1}^{-1}\n",
    "\\begin{bmatrix}\\bm{t}\\\\ t'\\end{bmatrix} \\right).\n",
    "$$\n",
    "\n",
    "In principle, we are now done, since the mean and variance of this (single variable) Gaussian distribution characterize the predictive distribution entirely.  It is possible however to evaluate $\\bm{C}_{N+1}$ analytically in terms of $\\bm{C}_N$, the covariance matrix of just the training data.\n",
    "\n",
    "Using the notation from MacKay, an additional observation corresponds to adding an additional row and column to the covariance matrix:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\bm{C}_{N+1} = \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        & & \\\\\n",
    "        & \\bm{C}_N & \\\\\n",
    "        & &\n",
    "    \\end{bmatrix}  &  \n",
    "    \\!\\!\\!\\begin{bmatrix}\n",
    "        \\\\\n",
    "        \\bm{k} \\\\\n",
    "        {}\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix}\n",
    "        \\, & \\bm{k}^\\T & \\,\n",
    "    \\end{bmatrix} &\n",
    "    \\!\\!\\!\\begin{bmatrix}\n",
    "        \\,\\kappa\\,\n",
    "    \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}.\n",
    "$$\n",
    "\n",
    "The inverse of this block matrix can be computed directly using the identity given below, and for this case it turns out that\n",
    "\n",
    "$$\n",
    "\\newcommand{\\m}{\\mathscr{m}}\n",
    "\\bm{C}_{N+1}^{-1} =\n",
    "\\left[\\!\\!\n",
    "\\begin{array}{c|c}\n",
    "    \\bm{M} & \\bm{m} \\\\\n",
    "    \\hline\n",
    "    \\bm{m} & \\m\n",
    "\\end{array}\n",
    "\\!\\!\\right]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\newcommand{\\Ci}{\\bm{C}_N^{-1}}\n",
    "\\mathscr{m} &= \\left(\\kappa - \\bm{k}^\\T\\Ci\\bm{k}\\right)^{-1}\\\\\n",
    "\\bm{m} &= -\\m \\Ci\\\\\n",
    "\\bm{M} &= \\Ci + \\frac{1}{\\m}\\bm{m}\\bm{m}^\\T.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This allows the predictive mean and variance to be read off as\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{align}\n",
    "\\bar{t}' &= \\bm{k}^\\T\\Ci\\bm{t}\\\\\n",
    "\\Var(t') &= \\kappa - \\bm{k}^\\T\\Ci\\bm{k}.\n",
    "\\end{align}\n",
    "}\n",
    "$$\n",
    "\n",
    "We can obtain a similar expression for the case of multiple dependent predictions, see the section **Sampling from the process**.\n",
    "\n",
    "### Matrix identity\n",
    "\n",
    "We used the following identity for the inverse of a block-partitioned matrix:\n",
    "\n",
    "$$\n",
    "\\newcommand{\\A}{\\bm{A}}\n",
    "\\newcommand{\\B}{\\bm{B}}\n",
    "\\newcommand{\\C}{\\bm{C}}\n",
    "\\newcommand{\\D}{\\bm{D}}\n",
    "\\newcommand{\\inv}[1]{#1^{-1}}\n",
    "\\left[\\!\\!\n",
    "\\begin{array}{c|c}\n",
    "    \\bm{A} & \\bm{B} \\\\\n",
    "    \\hline\n",
    "    \\bm{C} & \\bm{D}\n",
    "\\end{array}\n",
    "\\!\\!\\right]^{-1} =\n",
    "\\left[\\!\\!\n",
    "\\begin{array}{c|c}\n",
    "    \\inv{(\\A-\\B\\inv\\D\\C)} & -\\inv{(\\A - \\B\\inv\\D\\C)}\\B\\inv\\D \\\\\n",
    "    \\hline\n",
    "    -\\inv\\D\\C\\inv{(\\A - \\B\\inv\\D\\C)} & \\inv\\D + \\inv\\D\\C\\inv{(\\A-\\B\\inv\\D\\C)}\\B\\inv\\D\n",
    "\\end{array}\n",
    "\\!\\!\\right]\n",
    "$$\n",
    "\n",
    "## Cost\n",
    "\n",
    "- **Training** is somewhat better than $O(N^3)$ multiplications, from inverting $\\bm{C}_N$.\n",
    "- **Predictive mean** calculation from an already-trained process is $O(N)$ (a single dot-product of two length-$N$ vectors)\n",
    "- **predictive variance** calculation is $O(N^2)$.\n",
    "\n",
    "## Representing Gaussian Processes\n",
    "\n",
    "Now let us define a type representing a Gaussian process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct GP\n",
    "    # The covariance function\n",
    "    cov_kernel :: Function\n",
    "    # The noise model\n",
    "    ν          :: Float64\n",
    "    # The hyperparameters\n",
    "    θ          :: Vector{Float64}\n",
    "    # The inputs and observations\n",
    "    xs         :: Vector\n",
    "    ts         :: Vector{Float64}\n",
    "    # Covariance matrix (and its inverse)\n",
    "    C          :: Matrix{Float64}\n",
    "    # Inverse of covariance matrix (but see note)\n",
    "    invC       :: Matrix{Float64}\n",
    "    # Precomputed C\\ts\n",
    "    invCts     :: Vector{Float64}\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a constructor for the GP struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function GP(cov_kernel, ν, θ, xs=Array{Float64}(0), ts=Array{Float64}(0))\n",
    "    # compute the covariance matrix\n",
    "    C = [cov_kernel(x1, x2, θ...) for x1=xs, x2=xs] + diagm([ν for x=xs])\n",
    "    # defer to the default constructor\n",
    "    GP(cov_kernel, ν, θ, xs, ts, C, inv(C), C \\ ts)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GP object can now be constructed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gp = GP(squared_exp, 1E-7, [4.0, 20.0], [1.0, 2.0, 3.0], [1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do something interesting with it, we define some methods on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar(gp :: GP, x1, x2) = gp.cov_kernel(x1, x2, gp.θ...)\n",
    "\n",
    "\n",
    "# Julia feature: M' == transpose(M)\n",
    "function gp_predict(gp :: GP, xnew :: Float64)\n",
    "    k = [covar(gp, xnew, x) for x = gp.xs]\n",
    "    kappa = covar(gp, xnew, xnew) + gp.ν\n",
    "    mean = k' * gp.invCts\n",
    "    var = kappa - k' * gp.invC * k\n",
    "    return (mean, var)\n",
    "end\n",
    "\n",
    "\"\"\"Given a Gaussian process and a vector of input vectors to predict, \n",
    "return a tuple consisting of the mean and covariance of the posterior \n",
    "distribution at those points\"\"\"\n",
    "function gp_predict(gp :: GP, xnews :: Vector)\n",
    "    k = [covar(gp, xnew, x) for x = gp.xs, xnew = xnews]\n",
    "    kappa = [covar(gp, xnew1, xnew2) for xnew1 in xnews, xnew2 in xnews] + diagm([gp.ν for xnew in xnews])\n",
    "    means = k' * gp.invCts\n",
    "    vars = kappa - k' * gp.invC * k\n",
    "    return (means, vars)\n",
    "end\n",
    "\n",
    "\"Update the hyperparameters in place\"\n",
    "function set_hyperparameters!(gp :: GP, hypers :: Vector{Float64})\n",
    "    if (gp.ν != hypers[1]) || (gp.θ != hypers[2:end])\n",
    "        # We defined '←' above: performs the update in place\n",
    "        gp ← GP(gp.cov_kernel, hypers[1], hypers[2:end], gp.xs, gp.ts)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Julia features: DataFrames, anonymous functions, random numbers\n",
    "\n",
    "using DataFrames\n",
    "\n",
    "let xs = linspace(0.0, 1.0, 11);\n",
    "    global synth1 = DataFrame(x = xs, \n",
    "                              t = map(x -> x + 0.1*randn(), xs))\n",
    "end\n",
    "\n",
    "let xs = linspace(0.0, 1.0, 21);\n",
    "    global synth2 = DataFrame(x = xs,\n",
    "                              t = map(x -> sin(7*x) + 0.5*sin(9*x) + 1.2*sin(11*x), xs))\n",
    "end\n",
    "\n",
    "synth3 = DataFrame(x = [1,2,3,4,5], t = [1.2, 2.0, 0.0, 4.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Julia features: symbols, quoting\n",
    "draw(SVGJS(24cm, 8cm),\n",
    "    hstack(plot(synth1, x=:x, y=:t), plot(synth2, x=:x, y=:t), plot(synth3, x=:x, y=:t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive distributions on some of the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Julia features: convert, ribbon plots from a DataFrame\n",
    "\n",
    "synth2_gp = GP(squared_exp, 1E-6, [4.0, 0.2], convert(Array, synth2[:x]), convert(Array, synth2[:t]))\n",
    "begin\n",
    "    xs = linspace(0.0,2.0,100)\n",
    "    ts_it, vars_it = zip([gp_predict(synth2_gp, x) for x in xs]...)\n",
    "    ts = [ts_it...]\n",
    "    vars = [vars_it...]\n",
    "    global synth2_fit = DataFrame(\n",
    "                x = xs,\n",
    "                t = ts,\n",
    "                tmin = ts - sqrt.(vars),\n",
    "                tmax = ts + sqrt.(vars)\n",
    "           )\n",
    "end\n",
    "plot(layer(synth2, x=:x, y=:t, Geom.point),\n",
    "     layer(synth2_fit, x=:x, y=:t, ymin=:tmin, ymax=:tmax, Geom.line, Geom.ribbon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that far away from the data, the posterior process tends to the mean ($0$) and standard deviation ($\\sigma = 4$) of the prior process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth3_gp = GP(squared_exp, 1E-6, [4.0, 1.0], convert(Array, synth3[:x]), convert(Array, synth3[:t]))\n",
    "begin\n",
    "    xs = linspace(0.0,6.0,100)\n",
    "    ts_it, vars_it = zip([gp_predict(synth3_gp, x) for x in xs]...)\n",
    "    ts = [ts_it...]\n",
    "    vars = [vars_it...]\n",
    "    global synth3_fit = DataFrame(\n",
    "                x = xs,\n",
    "                t = ts,\n",
    "                tmin = ts - sqrt.(vars),\n",
    "                tmax = ts + sqrt.(vars)\n",
    "           )\n",
    "end\n",
    "plot(layer(synth3, x=:x, y=:t, Geom.point),\n",
    "     layer(synth3_fit, x=:x, y=:t, ymin=:tmin, ymax=:tmax, Geom.line, Geom.ribbon),  Coord.cartesian(ymax=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\bm{t}^*$ be a sample of $m$ elements from the Gaussian process that we wish to take, with inputs $\\bm{x}^{*(i)}$ $(1\\le i \\le m)$.\n",
    "\n",
    "Sampling a vector from the Gaussian process prior is just sampling from the multivariate normal distribution with covariance corresponding to the sample inputs $\\bm{x}^{*(i)}$:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\bm{t}^* \\sim N(\\bm{0},\\bm{\\kappa})\\\\\n",
    "\\kappa_{ij} = k(\\bm{x}^{*(i)}, \\bm{x}^{*(j)}) + \\nu\\delta_{ij}.\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Sampling from the posterior distribution uses the predictive distribution above, resulting in\n",
    "\n",
    "$$\n",
    "\\bm{t}^* \\sim N\\left(\\bm{K}^\\T \\bm{C}^{-1}\\bm{t},\\; \\bm\\kappa-\\bm{K}^\\T \\bm{C}^{-1} \\bm{K}\\right)\n",
    "$$\n",
    "\n",
    "where $\\bm{t}$ and $\\bm{C}$ are the observations and their covariances, $\\bm\\kappa$ is the same expression as above, $K_{ij} = k(\\bm{x}^{(i)},\\bm{x}^{*(j)})$ is an $n\\times m$ matrix ($n$ initial observations, and $m$ in the sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Julia has many built in distributions - MvNormal is a multivariate normal\n",
    "import Distributions: MvNormal\n",
    "\n",
    "# rand draws samples from the given distribution, with optional argument of the count\n",
    "rand(MvNormal([0.0,0.0], [1.0 0.0; 0.0 1.0]), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gp_sample_prior(gp :: GP, xs, count=1)\n",
    "    vars = [covar(gp, x1, x2) for x1 in xs, x2 in xs] + diagm([gp.ν for x in xs])\n",
    "    rand(MvNormal(zeros(xs), vars), count)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample from the GP prior we defined above, with $\\nu = 10^{-6}$, $\\sigma^2 = 4.0$ and $r = 1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let xs = linspace(0.0, 6.0, 100)\n",
    "    plot([layer(y = gp_sample_prior(synth3_gp, xs), x=xs, Geom.line) for i in 1:20]...)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's sample from the prior process with $\\nu = 1.0$, $\\sigma^2 = 4.0$ and $r = 5.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let xs = linspace(0.0, 6.0, 100)\n",
    "    plot([layer(y = gp_sample_prior(GP(squared_exp, 1.0, [4.0, 5.0]), xs),\n",
    "                x = xs,\n",
    "                Geom.point,\n",
    "                color = [i]) \n",
    "          for i in 1:5]..., Scale.color_discrete())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from a given Gaussian process at a vector of xs\n",
    "function gp_sample(gp :: GP, xs, count=1)\n",
    "    ts, vars = gp_predict(gp, xs)\n",
    "    vars = 0.5 * (vars + vars')\n",
    "    \n",
    "    rand(MvNormal(ts, vars), count)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let xs = linspace(0.0, 6.0, 100)\n",
    "    plot([layer(y = gp_sample(synth3_gp, Vector(xs)), x=xs, Geom.line) for i in 1:20]...,\n",
    "        layer(synth3_fit, x=:x, y=:t, ymin=:tmin, ymax=:tmax, Geom.line, Geom.ribbon),\n",
    "        Coord.cartesian(ymin = -5.0, ymax = 7.0))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the other choice of hyperparameters above, the posterior is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth3_gp2 = GP(squared_exp, 1.0, [4.0, 5.0], convert(Array, synth3[:x]), convert(Array, synth3[:t]))\n",
    "begin\n",
    "    xs = linspace(0.0,6.0,100)\n",
    "    ts_it, vars_it = zip([gp_predict(synth3_gp2, x) for x in xs]...)\n",
    "    ts = [ts_it...]\n",
    "    vars = [vars_it...]\n",
    "    global synth3_fit = DataFrame(\n",
    "                x = xs,\n",
    "                t = ts,\n",
    "                tmin = ts - sqrt.(vars),\n",
    "                tmax = ts + sqrt.(vars)\n",
    "           )\n",
    "end\n",
    "\n",
    "let xs = linspace(0.0, 6.0, 7)\n",
    "    plot(layer(synth3, x=:x, y=:t, Geom.point, Theme(default_color=\"black\", point_size=Measures.Length{:mm,Float64}(1.3)), shape=[xcross]),\n",
    "        [layer(y = gp_sample(synth3_gp2, Vector(xs)), x=xs, Geom.point, color=[i]) for i in 1:5]...,\n",
    "        layer(synth3_fit, x=:x, y=:t, ymin=:tmin, ymax=:tmax, Geom.line, Geom.ribbon),\n",
    "        Coord.cartesian(ymin = -5.0, ymax = 7.0), Scale.color_discrete())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the hyperparameters\n",
    "\n",
    "So far we have fixed a set of hyperparameters and made predictions from these.  As the number of training points increases, the hyperparameters become less significant for prediction, but this is not that useful in most cases.\n",
    "\n",
    "Instead of choosing a single value of the hyperparameters, which are usually unknown, it is better to marginalize over them.  That is, to compute\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\Pr(t' \\mid \\bm{x}', \\bm{X}, \\bm{t}) = \\int_\\Theta \\Pr(t' \\mid \\bm{x}', \\bm{X}, \\bm{t}, \\Theta) \\, \\Pr(\\Theta \\mid \\bm{X}, \\bm{t}) \\; \\text{d}\\Theta.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This could be computed using Monte Carlo integration (see the DSS session on MCMC), but often this integral is dominated by the most likely value of $\\Theta$:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\Pr(t' \\mid \\bm{x}', \\bm{X}, \\bm{t}) \\approx \\Pr(t' \\mid \\bm{x}', \\bm{X}, \\bm{t}, \\Theta_{\\text{MP}})\\\\\n",
    "\\Theta_{\\text{MP}} = \\argmax_\\Theta \\Pr(\\Theta \\mid \\bm{X}, \\bm{t}).\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "The posterior probability of $\\Theta$ is\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\Pr(\\Theta \\mid \\bm{X}, \\bm{t}) \\propto \\Pr(\\bm{t} \\mid \\bm{X}, \\Theta)\\,\\Pr(\\Theta).\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "It is usually most convenient to work with the logarithm of the liklihood.  The logarithm of the first term on the RHS is\n",
    "\n",
    "$$\n",
    "\\log \\Pr(\\bm{t} \\mid \\bm{X}, \\Theta) = -\\half \\log\\det \\bm{C} - \\half \\bm{t}^\\T \\bm{C}^{-1} \\bm{t} - \\frac{N}{2} \\log{2\\pi}.\n",
    "$$\n",
    "\n",
    "We can maximize this using an algorithm of our choice.  An analytic expression for the gradient is reasonably straightforward, or automatic differentiation could be used, but below I will use a derivative-free optimization method for simplicity, at the cost of slower convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical note: logdet computes the logarithm of the determinant directly and stably\n",
    "function log_lik(gp::GP)\n",
    "    -0.5 * (logdet(gp.C) + gp.ts' * gp.invC * gp.ts)[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Wrapper function around `log_lik` to pass to the optimizer.\"\n",
    "function log_lik!(gp::GP, hypers::Vector, grad::Vector = []; verbose::Bool = false)\n",
    "    set_hyperparameters!(gp, hypers)\n",
    "\n",
    "    ll = log_lik(gp)\n",
    "    \n",
    "    strify(vec) = chomp(string(vec'))\n",
    "    if verbose\n",
    "        print(\"log_lik! : ν=$(hypers[1])\\t θ=$(strify(hypers[2:end]))\\t log(likelihood)=$ll\\n\")\n",
    "    end\n",
    "    \n",
    "    return ll\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that actually performs the optimization.  Uses the [NLopt library](https://nlopt.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLopt\n",
    "\n",
    "function log_lik_optim!(gp::GP, lbounds, ubounds; iterations=20, ftol_rel=1e-2)\n",
    "    f(hypers, grad_unused) = log_lik!(gp, hypers, grad_unused, verbose=true)\n",
    "    opt = Opt(:LN_BOBYQA, length(gp.ν) + length(gp.θ))\n",
    "    max_objective!(opt, f)\n",
    "    lower_bounds!(opt, lbounds)\n",
    "    upper_bounds!(opt, ubounds)\n",
    "    ftol_rel!(opt, ftol_rel)\n",
    "    maxeval!(opt, iterations)\n",
    "    (maxf, maxx, ret) = optimize(opt, vcat(gp.ν, gp.θ))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize the hyperparameters of our 'synth3' example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy since log_lik! modifies its argument\n",
    "let synth3_gp_optim = deepcopy(synth3_gp)\n",
    "    log_lik_optim!(synth3_gp_optim, [1.0e-9, 1.0, 0.1], [1.0, 5.0, 10.0], iterations=20, ftol_rel=1E-5)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reproduces Figure 45.2(c) in MacKay's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let synth3_gp_plot = deepcopy(synth3_gp)\n",
    "    σ = 2.94 # the value for σ found in the optimization above, which is fixed for the plot\n",
    "    plot(z=(x,y)->log_lik!(synth3_gp_plot, [y, σ, x]; verbose=false), \n",
    "         y=linspace(0.0,4.0,100), \n",
    "         x=linspace(0.0,6.0,100), \n",
    "         Geom.contour(levels=[linspace(-7.8,-6.8,100)...]),\n",
    "         Coord.cartesian(xmin = 0.0, xmax = 6.0,\n",
    "                         ymin = 0.0, ymax = 4.0),\n",
    "         Guide.xlabel(\"lengthscale (r)\"),\n",
    "         Guide.ylabel(\"noise (ν)\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two local maxima.  One has low noise and is close to the parameters used in the `synth3_gp` example above, and the other has higher noise and is close to `synth3_gp2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- Much of this was following \"Information Theory, Inference and Learning Algorithms\" by David MacKay, available online - http://www.inference.org.uk/itprnn/book.html\n",
    "- Collection of various resources, including a comparison of libraries/software - http://www.gaussianprocess.org\n",
    "- Accessible online lecture notes on stochastic processes - https://fabricebaudoin.wordpress.com/2012/03/23/introductionbm/\n",
    "- Existing Julia packages for GPs: there seem to be several, and I haven't tried any of them.  This demonstration could have begun `Pkg.add(\"GaussianProcesses\")`...\n",
    "\n",
    "## Other topics\n",
    "\n",
    "- Heteroskedacity\n",
    "- Reduced-rank GPs\n",
    "- Co-kriging\n",
    "- Many simpler models can be formulated as GPs, including linear regression, spline fitting and a certain class of neural network.  See Ch 45 in MacKay for these examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
